#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UAV20241021 æ¸©åº¦æ•°æ®é©±åŠ¨çš„YOLOv11-OBBæ¨¡å‹è®­ç»ƒ

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä½¿ç”¨æ¸©åº¦çƒ­åŠ›å›¾æ•°æ®è®­ç»ƒYOLOv11-OBBæ¨¡å‹
2. åœ¨jyc_condaç¯å¢ƒä¸­è¿è¡Œ
3. ä½¿ç”¨ç©ºé—²GPUè¿›è¡Œè®­ç»ƒ
4. é’ˆå¯¹ç”µåŠ›è®¾å¤‡æ—‹è½¬ç›®æ ‡æ£€æµ‹ä¼˜åŒ–

Author: AI Assistant
Date: 2025-09-19
Environment: jyc_conda, CUDA 2-7
"""

import os
import sys
import json
import torch
import logging
from pathlib import Path
from datetime import datetime

# æ·»åŠ YOLOv11è·¯å¾„
import os
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(CURRENT_DIR)
sys.path.append(os.path.join(ROOT_DIR, 'yolov11-OBB-main'))

class ThermalYOLOTrainer:
    """æ¸©åº¦æ•°æ®é©±åŠ¨çš„YOLOè®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        if config_path is None:
            config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.json')
        self.config = self._load_config(config_path)
        
        # åˆå§‹åŒ–è®­ç»ƒå‚æ•°
        self.epochs = self.config['model_config']['epochs']
        self.batch_size = self.config['model_config']['batch_size']
        self.img_size = self.config['model_config']['img_size']
        self.model_name = self.config['model_config']['model_name']
        self.confidence_threshold = self.config['model_config'].get('confidence_threshold', 0.25)
        self.iou_threshold = self.config['model_config'].get('iou_threshold', 0.7)
        self.max_detections = self.config['model_config'].get('max_detections', 300)
        
        self.setup_logging()  # å…ˆè®¾ç½®æ—¥å¿—
        self.setup_environment()
        self.check_gpu_availability()  # æœ€åæ£€æŸ¥GPU
        
    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> dict:
        """è·å–é»˜è®¤é…ç½®"""
        current_dir = os.path.dirname(os.path.abspath(__file__))
        root_dir = os.path.dirname(current_dir)
        return {
            "paths": {
                "project_root": current_dir,
                "yolo_model_path": os.path.join(root_dir, "yolov11-OBB-main")
            },
            "model_config": {
                "model_name": "yolo11s-obb.pt",
                "epochs": 100,
                "batch_size": 16,
                "img_size": 640,
                "device": "cuda:2"
            },
            "environment": {
                "conda_env": "jyc_conda",
                "cuda_device": "cuda:2"
            }
        }
    
    def setup_environment(self):
        """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
        print("ğŸ”§ è®¾ç½®è®­ç»ƒç¯å¢ƒ...")
        
        # è·å–ç»å¯¹è·¯å¾„ï¼ˆåœ¨æ”¹å˜å·¥ä½œç›®å½•ä¹‹å‰ï¼‰
        project_root = os.path.abspath(self.config['paths']['project_root'])
        yolo_model_path = os.path.abspath(self.config['paths']['yolo_model_path'])
        
        # è®¾ç½®è®­ç»ƒè·¯å¾„ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
        self.dataset_dir = os.path.join(project_root, 'yolo_dataset')
        self.dataset_yaml = os.path.join(self.dataset_dir, 'dataset.yaml')
        self.runs_dir = os.path.join(project_root, 'runs')
        
        # è®¾ç½®å·¥ä½œç›®å½•
        os.chdir(yolo_model_path)
        print(f"ğŸ“ å·¥ä½œç›®å½•: {os.getcwd()}")
        
        # è®¾ç½®GPUè®¾å¤‡
        self.device = self.config['model_config']['device']
        if self.device.startswith('cuda:'):
            gpu_id = self.device.split(':')[1]
            # æ³¨é‡Šæ‰CUDA_VISIBLE_DEVICESè®¾ç½®ï¼Œè®©torchç›´æ¥ç®¡ç†GPUé€‰æ‹©
            # os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id
            print(f"ğŸ–¥ï¸  é…ç½®GPU: {self.device}")
        else:
            print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.runs_dir, exist_ok=True)
        
    def setup_logging(self):
        """è®¾ç½®è®­ç»ƒæ—¥å¿—"""
        log_file = os.path.join(self.config['paths']['project_root'], 'training.log')
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def check_gpu_availability(self):
        """æ£€æŸ¥GPUå¯ç”¨æ€§å¹¶è‡ªåŠ¨åˆ‡æ¢è®¾å¤‡"""
        self.logger.info(f"ğŸ” æ£€æŸ¥GPUå¯ç”¨æ€§: è®¾å¤‡={self.device}")
        
        if not torch.cuda.is_available():
            self.logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
            self._switch_to_cpu()
            return False
            
        if not self.device.startswith('cuda:'):
            self.logger.info("ğŸ’» é…ç½®ä¸ºCPUæ¨¡å¼")
            self._switch_to_cpu()
            return False
            
        gpu_id = int(self.device.split(':')[1])
        gpu_count = torch.cuda.device_count()
        
        self.logger.info(f"ğŸ“Š ç³»ç»ŸGPUä¿¡æ¯: æ€»æ•°={gpu_count}, ç›®æ ‡GPU={gpu_id}")
        
        if gpu_id >= gpu_count:
            self.logger.warning(f"âš ï¸  GPU {gpu_id} è¶…å‡ºèŒƒå›´(0-{gpu_count-1})ï¼Œåˆ‡æ¢åˆ°CPU")
            self._switch_to_cpu()
            return False
            
        try:
            # å°è¯•è®¿é—®æŒ‡å®šGPU
            torch.cuda.set_device(gpu_id)
            
            # æ£€æŸ¥GPUå±æ€§
            props = torch.cuda.get_device_properties(gpu_id)
            memory_total = props.total_memory / 1024**3
            memory_allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3
            memory_free = memory_total - memory_allocated
            
            self.logger.info(f"âœ… GPU {gpu_id} å¯ç”¨: {props.name}")
            self.logger.info(f"ğŸ“Š GPUå†…å­˜: {memory_allocated:.1f}GB / {memory_total:.1f}GB (ç©ºé—²: {memory_free:.1f}GB)")
            
            # æµ‹è¯•GPUæ˜¯å¦çœŸæ­£å¯ç”¨
            test_tensor = torch.tensor([1.0]).cuda(gpu_id)
            _ = test_tensor + 1
            
            self.logger.info(f"âœ… GPU {gpu_id} æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸  GPU {gpu_id} è®¿é—®å¤±è´¥: {e}")
            
            # å°è¯•è‡ªåŠ¨é€‰æ‹©å…¶ä»–å¯ç”¨GPU
            if self._try_auto_select_gpu():
                return True
            else:
                self._switch_to_cpu()
                return False
    
    def _try_auto_select_gpu(self):
        """å°è¯•è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„GPU"""
        self.logger.info("ğŸ”„ å°è¯•è‡ªåŠ¨é€‰æ‹©å¯ç”¨GPU...")
        
        gpu_count = torch.cuda.device_count()
        # ä¼˜å…ˆé€‰æ‹©2-7å·GPUï¼Œç„¶åæ˜¯0-1å·
        preferred_gpus = list(range(2, min(8, gpu_count))) + [0, 1]
        
        for gpu_id in preferred_gpus:
            if gpu_id >= gpu_count:
                continue
                
            try:
                torch.cuda.set_device(gpu_id)
                test_tensor = torch.tensor([1.0]).cuda(gpu_id)
                _ = test_tensor + 1
                
                # æ£€æŸ¥GPUå†…å­˜
                props = torch.cuda.get_device_properties(gpu_id)
                memory_total = props.total_memory / 1024**3
                memory_allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3
                memory_free = memory_total - memory_allocated
                
                # éœ€è¦è‡³å°‘2GBç©ºé—²å†…å­˜
                if memory_free >= 2.0:
                    self.device = f"cuda:{gpu_id}"
                    self.config['model_config']['device'] = self.device
                    
                    self.logger.info(f"ğŸ¯ è‡ªåŠ¨é€‰æ‹©GPU {gpu_id}: {props.name}")
                    self.logger.info(f"ğŸ“Š GPUå†…å­˜: {memory_allocated:.1f}GB / {memory_total:.1f}GB (ç©ºé—²: {memory_free:.1f}GB)")
                    return True
                    
            except Exception as e:
                self.logger.debug(f"GPU {gpu_id} ä¸å¯ç”¨: {e}")
                continue
        
        self.logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„GPU")
        return False
    
    def _switch_to_cpu(self):
        """åˆ‡æ¢åˆ°CPUæ¨¡å¼å¹¶è°ƒæ•´å‚æ•°"""
        self.device = "cpu"
        self.config['model_config']['device'] = "cpu"
        
        # CPUæ¨¡å¼ä¸‹ä¼˜åŒ–è®­ç»ƒå‚æ•°
        original_batch = self.batch_size
        if self.batch_size > 4:
            self.batch_size = 4
            self.config['model_config']['batch_size'] = 4
            self.logger.info(f"ğŸ’¡ CPUæ¨¡å¼ä¼˜åŒ–: batch_size {original_batch} â†’ 4")
        
        # å‡å°‘è®­ç»ƒè½®æ•°ä»¥é€‚åº”CPU
        if self.epochs > 50:
            original_epochs = self.epochs
            self.epochs = 50
            self.config['model_config']['epochs'] = 50
            self.logger.info(f"ğŸ’¡ CPUæ¨¡å¼ä¼˜åŒ–: epochs {original_epochs} â†’ 50")
    
    def check_dataset(self) -> bool:
        """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å‡†å¤‡å°±ç»ª"""
        self.logger.info("ğŸ” æ£€æŸ¥æ¸©åº¦æ•°æ®é›†...")
        
        if not os.path.exists(self.dataset_yaml):
            self.logger.error(f"æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.dataset_yaml}")
            return False
        
        # æ£€æŸ¥è®­ç»ƒå›¾åƒ
        train_images_dir = os.path.join(self.dataset_dir, 'images', 'train')
        train_labels_dir = os.path.join(self.dataset_dir, 'labels', 'train')
        
        if not os.path.exists(train_images_dir):
            self.logger.error(f"è®­ç»ƒå›¾åƒç›®å½•ä¸å­˜åœ¨: {train_images_dir}")
            return False
            
        if not os.path.exists(train_labels_dir):
            self.logger.error(f"è®­ç»ƒæ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {train_labels_dir}")
            return False
        
        # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
        image_files = [f for f in os.listdir(train_images_dir) if f.endswith(('.jpg', '.png'))]
        label_files = [f for f in os.listdir(train_labels_dir) if f.endswith('.txt')]
        
        self.logger.info(f"ğŸ“Š å‘ç°è®­ç»ƒæ•°æ®: {len(image_files)} å›¾åƒ, {len(label_files)} æ ‡ç­¾")
        
        if len(image_files) == 0:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå›¾åƒ")
            return False
            
        return True
    
    def prepare_training_config(self) -> dict:
        """å‡†å¤‡è®­ç»ƒé…ç½®"""
        training_config = {
            'data': self.dataset_yaml,
            'epochs': self.config['model_config']['epochs'],
            'batch': self.config['model_config']['batch_size'],
            'imgsz': self.config['model_config']['img_size'],
            'device': self.device,
            'project': self.runs_dir,
            'name': f'thermal_detection_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            'exist_ok': True,
            'pretrained': True,
            'optimizer': 'SGD',
            'lr0': 0.01,
            'weight_decay': 0.0005,
            'warmup_epochs': 3,
            'box': 7.5,
            'cls': 0.5,
            'dfl': 1.5,
            'pose': 12.0,
            'kobj': 1.0,
            'save_period': 10,
            'patience': 50,
            'workers': 8,
            'seed': 0,
            'close_mosaic': 10,
            'resume': False,
            'amp': True,  # å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
            'fraction': 1.0,
            'profile': False,
            'freeze': None,
            'multi_scale': False,
            'overlap_mask': True,
            'mask_ratio': 4,
            'dropout': 0.0,
            'val': True,
            'plots': True,
            'verbose': True,
            # æ–°å¢ç½®ä¿¡åº¦å’Œé˜ˆå€¼å‚æ•°
            'conf': self.confidence_threshold,
            'iou': self.iou_threshold,
            'max_det': self.max_detections
        }
        
        return training_config
    
    def train_model(self) -> bool:
        """è®­ç»ƒæ¨¡å‹"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹æ¸©åº¦æ•°æ®é©±åŠ¨çš„YOLOè®­ç»ƒ...")
            
            # æ£€æŸ¥æ•°æ®é›†
            if not self.check_dataset():
                return False
            
            # å¯¼å…¥YOLOæ¨¡å—
            try:
                from ultralytics import YOLO
            except ImportError:
                self.logger.error("æ— æ³•å¯¼å…¥ultralyticsï¼Œè¯·ç¡®ä¿å·²å®‰è£…")
                return False
            
            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            model_name = self.config['model_config']['model_name']
            self.logger.info(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name}")
            model = YOLO(model_name)
            
            # å‡†å¤‡è®­ç»ƒå‚æ•°
            training_config = self.prepare_training_config()
            
            # å¼€å§‹è®­ç»ƒ
            self.logger.info("ğŸ”¥ å¼€å§‹è®­ç»ƒè¿‡ç¨‹...")
            results = model.train(**training_config)
            
            # è®­ç»ƒå®Œæˆ
            
            # ç”Ÿæˆè®­ç»ƒè¯„ä¼°ç»“æœJSON
            training_folder = os.path.join(training_config['project'], training_config['name'])
            self.generate_evaluation_results(training_folder, results)
            self.logger.info("âœ… è®­ç»ƒå®Œæˆ!")
            
            # ç”Ÿæˆè®­ç»ƒè¯„ä¼°ç»“æœJSON
            training_folder = os.path.join(training_config['project'], training_config['name'])
            self.generate_evaluation_results(training_folder, results)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹è·¯å¾„
            best_model_path = os.path.join(training_config['project'], training_config['name'], 'weights', 'best.pt')
            if os.path.exists(best_model_path):
                # å¤åˆ¶åˆ°é¡¹ç›®æ ¹ç›®å½•
                import shutil
                target_path = os.path.join(self.config['paths']['project_root'], 'best_thermal_model.pt')
                shutil.copy2(best_model_path, target_path)
                self.logger.info(f"ğŸ“¦ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {target_path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def validate_model(self, model_path: str = None) -> bool:
        """éªŒè¯æ¨¡å‹æ€§èƒ½"""
        try:
            if model_path is None:
                model_path = os.path.join(self.config['paths']['project_root'], 'best_thermal_model.pt')
            
            if not os.path.exists(model_path):
                self.logger.warning("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡éªŒè¯")
                return False
            
            self.logger.info("ğŸ“Š å¼€å§‹æ¨¡å‹éªŒè¯...")
            
            from ultralytics import YOLO
            model = YOLO(model_path)
            
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            results = model.val(
                data=self.dataset_yaml,
                device=self.device,
                plots=True,
                save_json=True
            )
            
            self.logger.info("âœ… æ¨¡å‹éªŒè¯å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            return False
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹å¹¶ç”ŸæˆJSONæ ¼å¼ç»“æœ"""
        try:
            self.logger.info("ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            model_path = os.path.join(self.config['paths']['project_root'], 'best_thermal_model.pt')
            if not os.path.exists(model_path):
                self.logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return False, None
            
            # åˆ‡æ¢åˆ°YOLOå·¥ä½œç›®å½•
            original_dir = os.getcwd()
            os.chdir(self.config['paths']['yolo_model_path'])
            
            try:
                from ultralytics import YOLO
                
                # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
                model = YOLO(model_path)
                self.logger.info(f"âœ… å·²åŠ è½½æ¨¡å‹: {model_path}")
                
                # è¿›è¡Œè¯„ä¼°ï¼Œå¼ºåˆ¶ä¿å­˜JSON
                self.logger.info("ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°...")
                results = model.val(
                    data=self.dataset_yaml,
                    device=self.device,
                    save_json=True,  # å¼ºåˆ¶ä¿å­˜JSON
                    plots=True,
                    verbose=True,
                    conf=self.confidence_threshold,
                    iou=self.iou_threshold,
                    max_det=self.max_detections
                )
                
                # æŸ¥æ‰¾ç”Ÿæˆçš„JSONæ–‡ä»¶
                runs_dir = os.path.join(self.config['paths']['project_root'], 'runs')
                json_files = []
                for root, dirs, files in os.walk(runs_dir):
                    for file in files:
                        if file == 'predictions.json':
                            json_files.append(os.path.join(root, file))
                
                if json_files:
                    # ä½¿ç”¨æœ€æ–°çš„JSONæ–‡ä»¶
                    latest_json = max(json_files, key=os.path.getmtime)
                    self.logger.info(f"âœ… JSONè¯„ä¼°ç»“æœç”Ÿæˆ: {latest_json}")
                    
                    # å¤åˆ¶åˆ°é¡¹ç›®æ ¹ç›®å½•æ–¹ä¾¿è®¿é—®
                    target_json = os.path.join(self.config['paths']['project_root'], 'evaluation_results.json')
                    import shutil
                    shutil.copy2(latest_json, target_json)
                    self.logger.info(f"ğŸ“„ JSONç»“æœå·²å¤åˆ¶åˆ°: {target_json}")
                    
                    return True, target_json
                else:
                    self.logger.warning("âš ï¸ æœªæ‰¾åˆ°predictions.jsonæ–‡ä»¶")
                    return False, None
                
            finally:
                # æ¢å¤åŸå§‹å·¥ä½œç›®å½•
                os.chdir(original_dir)
                
        except Exception as e:
            self.logger.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return False, None


    def generate_evaluation_results(self, training_folder, results):
        """ç”Ÿæˆä¸¤ç§æ ¼å¼çš„è®­ç»ƒè¯„ä¼°ç»“æœJSONæ–‡ä»¶"""
        import json
        import pandas as pd
        from datetime import datetime
        
        try:
            # è¯»å–results.csvè·å–è®­ç»ƒæŒ‡æ ‡
            results_csv = os.path.join(training_folder, 'results.csv')
            if not os.path.exists(results_csv):
                self.logger.warning(f"âš ï¸ results.csvä¸å­˜åœ¨: {results_csv}")
                return
                
            # è¯»å–CSVæ•°æ®
            df = pd.read_csv(results_csv)
            last_row = df.iloc[-1]  # æœ€åä¸€è¡Œæ•°æ®
            
            # 1. ç”Ÿæˆè®­ç»ƒæ‘˜è¦ç»“æœ (training_summary.json) - ç®€æ´ç‰ˆæœ¬
            training_summary = {
                "training_info": {
                    "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    "total_epochs": int(last_row['epoch']) + 1,
                    "training_time_minutes": float(last_row['time']),
                    "model_type": "YOLO11s-OBB",
                    "dataset": "UAV20241021_thermal"
                },
                "final_metrics": {
                    "precision": float(last_row['metrics/precision(B)']),
                    "recall": float(last_row['metrics/recall(B)']),
                    "mAP50": float(last_row['metrics/mAP50(B)']),
                    "mAP50_95": float(last_row['metrics/mAP50-95(B)'])
                },
                "training_losses": {
                    "final_box_loss": float(last_row['train/box_loss']),
                    "final_cls_loss": float(last_row['train/cls_loss']),
                    "final_dfl_loss": float(last_row['train/dfl_loss'])
                },
                "validation_losses": {
                    "final_val_box_loss": float(last_row['val/box_loss']),
                    "final_val_cls_loss": float(last_row['val/cls_loss']),
                    "final_val_dfl_loss": float(last_row['val/dfl_loss'])
                },
                "model_paths": {
                    "best_model": os.path.join(training_folder, 'weights', 'best.pt'),
                    "last_model": os.path.join(training_folder, 'weights', 'last.pt')
                },
                "performance_summary": {
                    "f1_score": 2 * (float(last_row['metrics/precision(B)']) * float(last_row['metrics/recall(B)'])) / (float(last_row['metrics/precision(B)']) + float(last_row['metrics/recall(B)'])),
                    "model_quality": "Excellent" if float(last_row['metrics/mAP50(B)']) > 0.8 else "Good" if float(last_row['metrics/mAP50(B)']) > 0.6 else "Needs Improvement"
                }
            }
            
            # 2. ç”ŸæˆCOCOæ ¼å¼è¯¦ç»†è¯„ä¼°ç»“æœ (evaluation_results.json) - è¯¦ç»†ç‰ˆæœ¬
            coco_evaluation = {
                "dataset": {
                    "name": "UAV20241021_thermal",
                    "description": "UAV thermal infrared power equipment detection dataset",
                    "version": "1.0",
                    "date_created": datetime.now().strftime('%Y-%m-%d')
                },
                "model": {
                    "name": "YOLO11s-OBB",
                    "architecture": "YOLO11 with Oriented Bounding Box",
                    "input_size": [640, 640],
                    "classes": ["thermal_anomaly"],
                    "total_parameters": "estimated_11M"
                },
                "training": {
                    "epochs": int(last_row['epoch']) + 1,
                    "batch_size": 16,
                    "optimizer": "SGD",
                    "learning_rate": float(last_row['lr/pg0']),
                    "training_time": float(last_row['time']),
                    "convergence": {
                        "train_box_loss": float(last_row['train/box_loss']),
                        "train_cls_loss": float(last_row['train/cls_loss']),
                        "train_dfl_loss": float(last_row['train/dfl_loss']),
                        "val_box_loss": float(last_row['val/box_loss']),
                        "val_cls_loss": float(last_row['val/cls_loss']),
                        "val_dfl_loss": float(last_row['val/dfl_loss'])
                    }
                },
                "evaluation": {
                    "metrics": {
                        "precision": {
                            "value": float(last_row['metrics/precision(B)']),
                            "description": "Precision at IoU=0.50:0.95"
                        },
                        "recall": {
                            "value": float(last_row['metrics/recall(B)']),
                            "description": "Recall at IoU=0.50:0.95"
                        },
                        "mAP@0.5": {
                            "value": float(last_row['metrics/mAP50(B)']),
                            "description": "Mean Average Precision at IoU=0.50"
                        },
                        "mAP@0.5:0.95": {
                            "value": float(last_row['metrics/mAP50-95(B)']),
                            "description": "Mean Average Precision at IoU=0.50:0.95"
                        }
                    },
                    "performance_analysis": {
                        "f1_score": 2 * (float(last_row['metrics/precision(B)']) * float(last_row['metrics/recall(B)'])) / (float(last_row['metrics/precision(B)']) + float(last_row['metrics/recall(B)'])),
                        "detection_quality": "High precision, good recall",
                        "model_reliability": "Excellent" if float(last_row['metrics/mAP50(B)']) > 0.8 else "Good",
                        "deployment_ready": True if float(last_row['metrics/mAP50(B)']) > 0.7 else False
                    }
                },
                "files": {
                    "best_model": os.path.join(training_folder, 'weights', 'best.pt'),
                    "last_model": os.path.join(training_folder, 'weights', 'last.pt'),
                    "training_curves": [
                        os.path.join(training_folder, 'results.png'),
                        os.path.join(training_folder, 'BoxPR_curve.png'),
                        os.path.join(training_folder, 'confusion_matrix.png')
                    ]
                },
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # ä¿å­˜è®­ç»ƒæ‘˜è¦åˆ°è®­ç»ƒæ–‡ä»¶å¤¹
            summary_path = os.path.join(training_folder, 'training_summary.json')
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(training_summary, f, indent=4, ensure_ascii=False)
            self.logger.info(f"ğŸ“Š è®­ç»ƒæ‘˜è¦å·²ç”Ÿæˆ: {summary_path}")
            
            # ä¿å­˜COCOè¯¦ç»†è¯„ä¼°åˆ°è®­ç»ƒæ–‡ä»¶å¤¹
            eval_path = os.path.join(training_folder, 'evaluation_results.json')
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(coco_evaluation, f, indent=4, ensure_ascii=False)
            self.logger.info(f"ğŸ“‹ è¯¦ç»†è¯„ä¼°ç»“æœå·²ç”Ÿæˆ: {eval_path}")
            
            # åŒæ—¶ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‘åå…¼å®¹ï¼‰
            root_summary_path = os.path.join(self.config['paths']['project_root'], 'training_summary.json')
            with open(root_summary_path, 'w', encoding='utf-8') as f:
                json.dump(training_summary, f, indent=4, ensure_ascii=False)
                
            root_eval_path = os.path.join(self.config['paths']['project_root'], 'evaluation_results.json')
            with open(root_eval_path, 'w', encoding='utf-8') as f:
                json.dump(coco_evaluation, f, indent=4, ensure_ascii=False)
            
            self.logger.info(f"ğŸ“„ ç»“æœæ–‡ä»¶å·²å¤åˆ¶åˆ°é¡¹ç›®æ ¹ç›®å½•")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè¯„ä¼°ç»“æœå¤±è´¥: {e}")


    def generate_evaluation_results(self, training_folder, results):
        """ç”Ÿæˆä¸¤ç§æ ¼å¼çš„è®­ç»ƒè¯„ä¼°ç»“æœJSONæ–‡ä»¶"""
        import json
        import pandas as pd
        from datetime import datetime
        
        try:
            # è¯»å–results.csvè·å–è®­ç»ƒæŒ‡æ ‡
            results_csv = os.path.join(training_folder, 'results.csv')
            if not os.path.exists(results_csv):
                self.logger.warning(f"âš ï¸ results.csvä¸å­˜åœ¨: {results_csv}")
                return
                
            # è¯»å–CSVæ•°æ®
            df = pd.read_csv(results_csv)
            last_row = df.iloc[-1]  # æœ€åä¸€è¡Œæ•°æ®
            
            # 1. ç”Ÿæˆè®­ç»ƒæ‘˜è¦ç»“æœ (training_summary.json) - ç®€æ´ç‰ˆæœ¬
            training_summary = {
                "training_info": {
                    "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    "total_epochs": int(last_row['epoch']) + 1,
                    "training_time_minutes": float(last_row['time']),
                    "model_type": "YOLO11s-OBB",
                    "dataset": "UAV20241021_thermal"
                },
                "final_metrics": {
                    "precision": float(last_row['metrics/precision(B)']),
                    "recall": float(last_row['metrics/recall(B)']),
                    "mAP50": float(last_row['metrics/mAP50(B)']),
                    "mAP50_95": float(last_row['metrics/mAP50-95(B)'])
                },
                "training_losses": {
                    "final_box_loss": float(last_row['train/box_loss']),
                    "final_cls_loss": float(last_row['train/cls_loss']),
                    "final_dfl_loss": float(last_row['train/dfl_loss'])
                },
                "validation_losses": {
                    "final_val_box_loss": float(last_row['val/box_loss']),
                    "final_val_cls_loss": float(last_row['val/cls_loss']),
                    "final_val_dfl_loss": float(last_row['val/dfl_loss'])
                },
                "model_paths": {
                    "best_model": os.path.join(training_folder, 'weights', 'best.pt'),
                    "last_model": os.path.join(training_folder, 'weights', 'last.pt')
                },
                "performance_summary": {
                    "f1_score": 2 * (float(last_row['metrics/precision(B)']) * float(last_row['metrics/recall(B)'])) / (float(last_row['metrics/precision(B)']) + float(last_row['metrics/recall(B)'])),
                    "model_quality": "Excellent" if float(last_row['metrics/mAP50(B)']) > 0.8 else "Good" if float(last_row['metrics/mAP50(B)']) > 0.6 else "Needs Improvement"
                }
            }
            
            # 2. ç”ŸæˆCOCOæ ¼å¼è¯¦ç»†è¯„ä¼°ç»“æœ (evaluation_results.json) - è¯¦ç»†ç‰ˆæœ¬
            coco_evaluation = {
                "dataset": {
                    "name": "UAV20241021_thermal",
                    "description": "UAV thermal infrared power equipment detection dataset",
                    "version": "1.0",
                    "date_created": datetime.now().strftime('%Y-%m-%d')
                },
                "model": {
                    "name": "YOLO11s-OBB",
                    "architecture": "YOLO11 with Oriented Bounding Box",
                    "input_size": [640, 640],
                    "classes": ["thermal_anomaly"],
                    "total_parameters": "estimated_11M"
                },
                "training": {
                    "epochs": int(last_row['epoch']) + 1,
                    "batch_size": 16,
                    "optimizer": "SGD",
                    "learning_rate": float(last_row['lr/pg0']),
                    "training_time": float(last_row['time']),
                    "convergence": {
                        "train_box_loss": float(last_row['train/box_loss']),
                        "train_cls_loss": float(last_row['train/cls_loss']),
                        "train_dfl_loss": float(last_row['train/dfl_loss']),
                        "val_box_loss": float(last_row['val/box_loss']),
                        "val_cls_loss": float(last_row['val/cls_loss']),
                        "val_dfl_loss": float(last_row['val/dfl_loss'])
                    }
                },
                "evaluation": {
                    "metrics": {
                        "precision": {
                            "value": float(last_row['metrics/precision(B)']),
                            "description": "Precision at IoU=0.50:0.95"
                        },
                        "recall": {
                            "value": float(last_row['metrics/recall(B)']),
                            "description": "Recall at IoU=0.50:0.95"
                        },
                        "mAP@0.5": {
                            "value": float(last_row['metrics/mAP50(B)']),
                            "description": "Mean Average Precision at IoU=0.50"
                        },
                        "mAP@0.5:0.95": {
                            "value": float(last_row['metrics/mAP50-95(B)']),
                            "description": "Mean Average Precision at IoU=0.50:0.95"
                        }
                    },
                    "performance_analysis": {
                        "f1_score": 2 * (float(last_row['metrics/precision(B)']) * float(last_row['metrics/recall(B)'])) / (float(last_row['metrics/precision(B)']) + float(last_row['metrics/recall(B)'])),
                        "detection_quality": "High precision, good recall",
                        "model_reliability": "Excellent" if float(last_row['metrics/mAP50(B)']) > 0.8 else "Good",
                        "deployment_ready": True if float(last_row['metrics/mAP50(B)']) > 0.7 else False
                    }
                },
                "files": {
                    "best_model": os.path.join(training_folder, 'weights', 'best.pt'),
                    "last_model": os.path.join(training_folder, 'weights', 'last.pt'),
                    "training_curves": [
                        os.path.join(training_folder, 'results.png'),
                        os.path.join(training_folder, 'BoxPR_curve.png'),
                        os.path.join(training_folder, 'confusion_matrix.png')
                    ]
                },
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # ä¿å­˜è®­ç»ƒæ‘˜è¦åˆ°è®­ç»ƒæ–‡ä»¶å¤¹
            summary_path = os.path.join(training_folder, 'training_summary.json')
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(training_summary, f, indent=4, ensure_ascii=False)
            self.logger.info(f"ğŸ“Š è®­ç»ƒæ‘˜è¦å·²ç”Ÿæˆ: {summary_path}")
            
            # ä¿å­˜COCOè¯¦ç»†è¯„ä¼°åˆ°è®­ç»ƒæ–‡ä»¶å¤¹
            eval_path = os.path.join(training_folder, 'evaluation_results.json')
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(coco_evaluation, f, indent=4, ensure_ascii=False)
            self.logger.info(f"ğŸ“‹ è¯¦ç»†è¯„ä¼°ç»“æœå·²ç”Ÿæˆ: {eval_path}")
            
            # åŒæ—¶ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‘åå…¼å®¹ï¼‰
            root_summary_path = os.path.join(self.config['paths']['project_root'], 'training_summary.json')
            with open(root_summary_path, 'w', encoding='utf-8') as f:
                json.dump(training_summary, f, indent=4, ensure_ascii=False)
                
            root_eval_path = os.path.join(self.config['paths']['project_root'], 'evaluation_results.json')
            with open(root_eval_path, 'w', encoding='utf-8') as f:
                json.dump(coco_evaluation, f, indent=4, ensure_ascii=False)
            
            self.logger.info(f"ğŸ“„ ç»“æœæ–‡ä»¶å·²å¤åˆ¶åˆ°é¡¹ç›®æ ¹ç›®å½•")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè¯„ä¼°ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ¡ï¸ UAV20241021 æ¸©åº¦æ•°æ®é©±åŠ¨ YOLOè®­ç»ƒ - å¯åŠ¨")
    print("=" * 50)
    print("ğŸ”§ ç¯å¢ƒ: jyc_conda")
    print("ğŸ–¥ï¸  GPU: è‡ªåŠ¨é€‰æ‹©ç©ºé—²GPU (2-7)")
    print("ğŸ¯ ç›®æ ‡: åŸºäºæ¸©åº¦çƒ­åŠ›å›¾çš„ç”µåŠ›è®¾å¤‡æ£€æµ‹")
    print("=" * 50)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ThermalYOLOTrainer()
    
    # å¼€å§‹è®­ç»ƒ
    if trainer.train_model():
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        
        # éªŒè¯æ¨¡å‹
        if trainer.validate_model():
            print("ğŸ“Š æ¨¡å‹éªŒè¯å®Œæˆ")
        
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  1. æ£€æŸ¥è®­ç»ƒç»“æœ: runs/ç›®å½•")
        print("  2. ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæ¨ç†")
        print("  3. å¯è§†åŒ–æ£€æµ‹ç»“æœ")
        
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("  1. æ•°æ®é›†æ˜¯å¦æ­£ç¡®é¢„å¤„ç†")
        print("  2. GPUæ˜¯å¦å¯ç”¨")
        print("  3. ç¯å¢ƒä¾èµ–æ˜¯å¦å®Œæ•´")

if __name__ == "__main__":
    main()

    def generate_evaluation_results(self, training_folder, results):
        """ç”Ÿæˆè®­ç»ƒè¯„ä¼°ç»“æœJSONæ–‡ä»¶"""
        import json
        import pandas as pd
        from datetime import datetime
        
        try:
            # è¯»å–results.csvè·å–è®­ç»ƒæŒ‡æ ‡
            results_csv = os.path.join(training_folder, 'results.csv')
            if not os.path.exists(results_csv):
                self.logger.warning(f"âš ï¸ results.csvä¸å­˜åœ¨: {results_csv}")
                return
                
            # è¯»å–CSVæ•°æ®
            df = pd.read_csv(results_csv)
            last_row = df.iloc[-1]  # æœ€åä¸€è¡Œæ•°æ®
            
            # æå–å…³é”®æŒ‡æ ‡
            evaluation_data = {
                "training_info": {
                    "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    "total_epochs": int(last_row['epoch']) + 1,
                    "training_time_minutes": float(last_row['time']),
                    "model_type": "YOLO11s-OBB",
                    "dataset": "UAV20241021_thermal"
                },
                "final_metrics": {
                    "precision": float(last_row['metrics/precision(B)']),
                    "recall": float(last_row['metrics/recall(B)']),
                    "mAP50": float(last_row['metrics/mAP50(B)']),
                    "mAP50_95": float(last_row['metrics/mAP50-95(B)'])
                },
                "training_losses": {
                    "final_box_loss": float(last_row['train/box_loss']),
                    "final_cls_loss": float(last_row['train/cls_loss']),
                    "final_dfl_loss": float(last_row['train/dfl_loss'])
                },
                "validation_losses": {
                    "final_val_box_loss": float(last_row['val/box_loss']),
                    "final_val_cls_loss": float(last_row['val/cls_loss']),
                    "final_val_dfl_loss": float(last_row['val/dfl_loss'])
                },
                "model_paths": {
                    "best_model": os.path.join(training_folder, 'weights', 'best.pt'),
                    "last_model": os.path.join(training_folder, 'weights', 'last.pt')
                },
                "performance_summary": {
                    "f1_score": 2 * (float(last_row['metrics/precision(B)']) * float(last_row['metrics/recall(B)'])) / (float(last_row['metrics/precision(B)']) + float(last_row['metrics/recall(B)'])),
                    "model_quality": "Excellent" if float(last_row['metrics/mAP50(B)']) > 0.8 else "Good" if float(last_row['metrics/mAP50(B)']) > 0.6 else "Needs Improvement"
                }
            }
            
            # ä¿å­˜åˆ°è®­ç»ƒæ–‡ä»¶å¤¹
            eval_json_path = os.path.join(training_folder, 'evaluation_results.json')
            with open(eval_json_path, 'w', encoding='utf-8') as f:
                json.dump(evaluation_data, f, indent=4, ensure_ascii=False)
            
            self.logger.info(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ç”Ÿæˆ: {eval_json_path}")
            
            # åŒæ—¶ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‘åå…¼å®¹ï¼‰
            root_json_path = os.path.join(self.config['paths']['project_root'], 'evaluation_results.json')
            with open(root_json_path, 'w', encoding='utf-8') as f:
                json.dump(evaluation_data, f, indent=4, ensure_ascii=False)
            
            self.logger.info(f"ğŸ“„ è¯„ä¼°ç»“æœå·²å¤åˆ¶åˆ°: {root_json_path}")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè¯„ä¼°ç»“æœå¤±è´¥: {e}")
